{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walkthrough used: \n",
    "- https://techs0uls.wordpress.com/2019/12/12/donut-unsupervised-anomaly-detection-using-vae/\n",
    "- https://github.com/NetManAIOps/donut\n",
    "- https://github.com/thu-ml/zhusuan\n",
    "- https://github.com/haowen-xu/tfsnippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from donut import complete_timestamp, standardize_kpi, Donut, DonutTrainer, DonutPredictor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as K\n",
    "from tfsnippet.modules import Sequential\n",
    "from tfsnippet.utils import get_variables_as_dict, VariableSaver\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "tf.disable_v2_behavior()\n",
    "import warnings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\Payton Rudnick\\Documents\\donut\\sample_data\\g.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1476460800</td>\n",
       "      <td>0.012604</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1476460860</td>\n",
       "      <td>0.017786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1476460920</td>\n",
       "      <td>0.012014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1476460980</td>\n",
       "      <td>0.017062</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1476461040</td>\n",
       "      <td>0.023632</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214879</th>\n",
       "      <td>1489420500</td>\n",
       "      <td>0.024097</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214880</th>\n",
       "      <td>1489420560</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214881</th>\n",
       "      <td>1489420620</td>\n",
       "      <td>0.008676</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214882</th>\n",
       "      <td>1489420680</td>\n",
       "      <td>0.029228</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214883</th>\n",
       "      <td>1489420740</td>\n",
       "      <td>0.029041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214884 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         timestamp     value  label\n",
       "0       1476460800  0.012604      0\n",
       "1       1476460860  0.017786      0\n",
       "2       1476460920  0.012014      0\n",
       "3       1476460980  0.017062      0\n",
       "4       1476461040  0.023632      0\n",
       "...            ...       ...    ...\n",
       "214879  1489420500  0.024097      0\n",
       "214880  1489420560  0.009231      0\n",
       "214881  1489420620  0.008676      0\n",
       "214882  1489420680  0.029228      0\n",
       "214883  1489420740  0.029041      0\n",
       "\n",
       "[214884 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    198771\n",
      "1     16113\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Seems like we have 209 'anomalies' and 17,359 'normal' datapoints\n",
    "print(data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = data['timestamp']\n",
    "values = data['value']\n",
    "labels = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps: 214884\n"
     ]
    }
   ],
   "source": [
    "print(\"Timestamps: {}\".format(timestamp.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Points: 1116\n",
      "Labeled Anomalies: 16113\n"
     ]
    }
   ],
   "source": [
    "#Complete the timestamp, and obtain the missing point indicators\n",
    "\n",
    "timestamp, missing, (values, labels) = complete_timestamp(timestamp, (values, labels))\n",
    "\n",
    "print(\"Missing Points: {}\".format(np.sum(missing == 1)))\n",
    "print(\"Labeled Anomalies: {}\".format(np.sum(labels == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in test set: 64800\n",
      "Anomalies in test set: 6334\n"
     ]
    }
   ],
   "source": [
    "#Spliting the train & test dataset \n",
    "\n",
    "#Setting the test portion of the dataset \n",
    "test_portion = 0.3\n",
    "\n",
    "test_n = int(len(values) * test_portion)\n",
    "\n",
    "train_values, test_values = values[:-test_n], values[-test_n:]\n",
    "\n",
    "train_labels, test_labels = labels[:-test_n], labels[-test_n:]\n",
    "\n",
    "train_missing, test_missing = missing[:-test_n], missing[-test_n:]\n",
    "\n",
    "print(\"Rows in test set: {}\".format(test_values.shape[0]))\n",
    "print(\"Anomalies in test set: {}\".format(np.sum(test_labels == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently 6,334 anomaly points in the test set out of 64,800 data points. Is around 9.77% which is way too many but that is a problem for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the training and testing data \n",
    "\n",
    "train_values, mean, std = standardize_kpi(train_values, excludes=np.logical_or(train_labels, train_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train values mean: 0.04438549280166626\n",
      "Train values std: 0.04308837652206421\n"
     ]
    }
   ],
   "source": [
    "test_values, _, _ = standardize_kpi(test_values, mean=mean, std=std)\n",
    "\n",
    "print(\"Train values mean: {}\".format(mean))\n",
    "print(\"Train values std: {}\".format(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the entire model within the scope of 'model_vs', it should hold exactly all the variables of 'model', including the variables created by the Keras layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the sliding window\n",
    "sliding_window = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('model') as model_vs:\n",
    "    model = Donut(\n",
    "        h_for_p_x=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        h_for_q_z=Sequential([\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n",
    "                           activation=tf.nn.relu),\n",
    "        ]),\n",
    "        x_dims=sliding_window,\n",
    "        z_dims=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it for prediction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters                     (58,150 in total)\n",
      "----------------------------------------------------------\n",
      "donut/p_x_given_z/x_mean/bias           (120,)         120\n",
      "donut/p_x_given_z/x_mean/kernel         (100, 120)  12,000\n",
      "donut/p_x_given_z/x_std/bias            (120,)         120\n",
      "donut/p_x_given_z/x_std/kernel          (100, 120)  12,000\n",
      "donut/q_z_given_x/z_mean/bias           (5,)             5\n",
      "donut/q_z_given_x/z_mean/kernel         (100, 5)       500\n",
      "donut/q_z_given_x/z_std/bias            (5,)             5\n",
      "donut/q_z_given_x/z_std/kernel          (100, 5)       500\n",
      "sequential/forward/_0/dense/bias        (100,)         100\n",
      "sequential/forward/_0/dense/kernel      (5, 100)       500\n",
      "sequential/forward/_1/dense_1/bias      (100,)         100\n",
      "sequential/forward/_1/dense_1/kernel    (100, 100)  10,000\n",
      "sequential_1/forward/_0/dense_2/bias    (100,)         100\n",
      "sequential_1/forward/_0/dense_2/kernel  (120, 100)  12,000\n",
      "sequential_1/forward/_1/dense_3/bias    (100,)         100\n",
      "sequential_1/forward/_1/dense_3/kernel  (100, 100)  10,000\n",
      "\n",
      "[Epoch 1/30, Step 100] step time: 0.00599s (Â±0.02631s); valid time: 0.2079s; loss: 74.6977 (Â±54.4232); valid loss: 38.423 (*)\n",
      "[Epoch 1/30, Step 200] step time: 0.003887s (Â±0.01612s); valid time: 0.1602s; loss: 3.0364 (Â±6.91642); valid loss: 27.2188 (*)\n",
      "[Epoch 1/30, Step 300] step time: 0.004077s (Â±0.01795s); valid time: 0.1796s; loss: -4.48943 (Â±5.67286); valid loss: 23.7492 (*)\n",
      "[Epoch 1/30, Step 400] step time: 0.00388s (Â±0.01619s); valid time: 0.1629s; loss: -7.06483 (Â±5.34562); valid loss: 21.9052 (*)\n",
      "[Epoch 2/30, Step 500, ETA 58.68s] step time: 0.004039s (Â±0.01737s); valid time: 0.1748s; loss: -9.41653 (Â±5.60135); valid loss: 18.0462 (*)\n",
      "[Epoch 2/30, Step 600, ETA 56.05s] step time: 0.003561s (Â±0.01241s); valid time: 0.125s; loss: -10.9015 (Â±5.43472); valid loss: 18.1365\n",
      "[Epoch 2/30, Step 700, ETA 55.05s] step time: 0.00413s (Â±0.01748s); valid time: 0.176s; loss: -12.7073 (Â±5.77948); valid loss: 16.6907 (*)\n",
      "[Epoch 2/30, Step 800, ETA 54.22s] step time: 0.004205s (Â±0.01793s); valid time: 0.1805s; loss: -14.6382 (Â±5.63704); valid loss: 15.3205 (*)\n",
      "[Epoch 3/30, Step 900, ETA 54.12s] step time: 0.004081s (Â±0.01722s); valid time: 0.1733s; loss: -14.9099 (Â±5.2825); valid loss: 13.76 (*)\n",
      "[Epoch 3/30, Step 1000, ETA 53.13s] step time: 0.003923s (Â±0.01703s); valid time: 0.1713s; loss: -15.8003 (Â±5.44575); valid loss: 13.2393 (*)\n",
      "[Epoch 3/30, Step 1100, ETA 52.25s] step time: 0.003967s (Â±0.01696s); valid time: 0.1707s; loss: -15.1153 (Â±5.73908); valid loss: 13.0314 (*)\n",
      "[Epoch 3/30, Step 1200, ETA 50.89s] step time: 0.003417s (Â±0.01112s); valid time: 0.112s; loss: -12.3056 (Â±5.49275); valid loss: 17.4217\n",
      "[Epoch 4/30, Step 1300, ETA 50.66s] step time: 0.003877s (Â±0.01616s); valid time: 0.1616s; loss: -14.175 (Â±5.5817); valid loss: 11.9637 (*)\n",
      "[Epoch 4/30, Step 1400, ETA 50s] step time: 0.004044s (Â±0.01741s); valid time: 0.1732s; loss: -16.9385 (Â±6.00141); valid loss: 10.1327 (*)\n",
      "[Epoch 4/30, Step 1500, ETA 48.97s] step time: 0.00349s (Â±0.01192s); valid time: 0.12s; loss: -14.9073 (Â±7.23202); valid loss: 11.6642\n",
      "[Epoch 4/30, Step 1600, ETA 47.99s] step time: 0.003471s (Â±0.01183s); valid time: 0.1171s; loss: -17.0426 (Â±6.53996); valid loss: 12.6652\n",
      "[Epoch 5/30, Step 1700, ETA 47.8s] step time: 0.00397s (Â±0.01631s); valid time: 0.1642s; loss: -18.7063 (Â±5.99813); valid loss: 9.68994 (*)\n",
      "[Epoch 5/30, Step 1800, ETA 46.92s] step time: 0.00342s (Â±0.01154s); valid time: 0.1161s; loss: -19.0056 (Â±5.57518); valid loss: 10.6372\n",
      "[Epoch 5/30, Step 1900, ETA 46.05s] step time: 0.003408s (Â±0.0108s); valid time: 0.1078s; loss: -14.4318 (Â±6.04327); valid loss: 11.6532\n",
      "[Epoch 5/30, Step 2000, ETA 45.22s] step time: 0.003316s (Â±0.01063s); valid time: 0.107s; loss: -15.6888 (Â±5.72646); valid loss: 24.9571\n",
      "[Epoch 6/30, Step 2100, ETA 44.78s] step time: 0.003404s (Â±0.01086s); valid time: 0.1083s; loss: -14.1454 (Â±7.02702); valid loss: 16.1289\n",
      "[Epoch 6/30, Step 2200, ETA 44.05s] step time: 0.003493s (Â±0.01155s); valid time: 0.1163s; loss: -14.914 (Â±5.84856); valid loss: 11.5448\n",
      "[Epoch 6/30, Step 2300, ETA 43.55s] step time: 0.003886s (Â±0.01581s); valid time: 0.1591s; loss: -15.7151 (Â±5.67287); valid loss: 9.57562 (*)\n",
      "[Epoch 6/30, Step 2400, ETA 43.03s] step time: 0.00388s (Â±0.0157s); valid time: 0.158s; loss: -16.2937 (Â±6.23634); valid loss: 9.46359 (*)\n",
      "[Epoch 7/30, Step 2500, ETA 42.56s] step time: 0.003251s (Â±0.01003s); valid time: 0.1s; loss: -17.0557 (Â±6.43703); valid loss: 12.78\n",
      "[Epoch 7/30, Step 2600, ETA 42.03s] step time: 0.003702s (Â±0.01475s); valid time: 0.1474s; loss: -16.8964 (Â±5.77683); valid loss: 9.08117 (*)\n",
      "[Epoch 7/30, Step 2700, ETA 41.53s] step time: 0.003845s (Â±0.01545s); valid time: 0.1555s; loss: -17.7653 (Â±5.78543); valid loss: 6.22125 (*)\n",
      "[Epoch 7/30, Step 2800, ETA 40.88s] step time: 0.003335s (Â±0.01086s); valid time: 0.1093s; loss: -17.724 (Â±6.07173); valid loss: 15.1974\n",
      "[Epoch 8/30, Step 2900, ETA 40.46s] step time: 0.003304s (Â±0.0107s); valid time: 0.1077s; loss: -17.7396 (Â±5.59365); valid loss: 8.53091\n",
      "[Epoch 8/30, Step 3000, ETA 39.85s] step time: 0.003421s (Â±0.01082s); valid time: 0.108s; loss: -16.9009 (Â±6.54431); valid loss: 15.1229\n",
      "[Epoch 8/30, Step 3100, ETA 39.23s] step time: 0.003391s (Â±0.01081s); valid time: 0.1089s; loss: -17.7549 (Â±5.96808); valid loss: 6.51573\n",
      "[Epoch 8/30, Step 3200, ETA 38.64s] step time: 0.00341s (Â±0.01081s); valid time: 0.1069s; loss: -19.1557 (Â±6.22999); valid loss: 6.96427\n",
      "[Epoch 9/30, Step 3300, ETA 38.21s] step time: 0.003348s (Â±0.01001s); valid time: 0.1008s; loss: -16.5174 (Â±7.25122); valid loss: 6.86266\n",
      "[Epoch 9/30, Step 3400, ETA 37.62s] step time: 0.003293s (Â±0.009992s); valid time: 0.1006s; loss: -18.3383 (Â±6.16698); valid loss: 7.67986\n",
      "[Epoch 9/30, Step 3500, ETA 37.04s] step time: 0.003239s (Â±0.01012s); valid time: 0.1019s; loss: -20.4872 (Â±5.65133); valid loss: 7.20668\n",
      "[Epoch 9/30, Step 3600, ETA 36.48s] step time: 0.003332s (Â±0.01033s); valid time: 0.104s; loss: -20.5491 (Â±5.43568); valid loss: 6.91243\n",
      "[Epoch 9/30, Step 3700, ETA 36.05s] step time: 0.003749s (Â±0.01499s); valid time: 0.1509s; loss: -20.724 (Â±5.90916); valid loss: 6.15945 (*)\n",
      "[Epoch 10/30, Step 3800, ETA 35.69s] step time: 0.00337s (Â±0.01143s); valid time: 0.115s; loss: -19.6924 (Â±5.79549); valid loss: 7.51978\n",
      "[Epoch 10/30, Step 3900, ETA 35.18s] step time: 0.00344s (Â±0.01172s); valid time: 0.118s; loss: -19.0498 (Â±6.58659); valid loss: 12.0437\n",
      "[Epoch 10/30, Step 4000, ETA 34.67s] step time: 0.003495s (Â±0.01177s); valid time: 0.1185s; loss: -16.4648 (Â±6.61464); valid loss: 9.31921\n",
      "[Epoch 10/30, Step 4100, ETA 34.18s] step time: 0.00354s (Â±0.01191s); valid time: 0.119s; loss: -16.5689 (Â±6.96708); valid loss: 6.21403\n",
      "[Epoch 10/30, Step 4120, ETA 34.03s] Learning rate decreased to 0.00075\n",
      "[Epoch 11/30, Step 4200, ETA 33.8s] step time: 0.003351s (Â±0.01087s); valid time: 0.1084s; loss: -18.2327 (Â±5.61941); valid loss: 12.5954\n",
      "[Epoch 11/30, Step 4300, ETA 33.3s] step time: 0.003376s (Â±0.01093s); valid time: 0.109s; loss: -18.987 (Â±6.09415); valid loss: 10.175\n",
      "[Epoch 11/30, Step 4400, ETA 32.81s] step time: 0.003377s (Â±0.01099s); valid time: 0.1107s; loss: -20.1402 (Â±5.48432); valid loss: 6.44271\n",
      "[Epoch 11/30, Step 4500, ETA 32.32s] step time: 0.00334s (Â±0.01093s); valid time: 0.109s; loss: -19.4398 (Â±6.97413); valid loss: 18.6685\n",
      "[Epoch 12/30, Step 4600, ETA 31.95s] step time: 0.003363s (Â±0.01023s); valid time: 0.103s; loss: -19.3475 (Â±7.09517); valid loss: 6.30711\n",
      "[Epoch 12/30, Step 4700, ETA 31.55s] step time: 0.00388s (Â±0.0158s); valid time: 0.157s; loss: -21.0702 (Â±5.7745); valid loss: 5.51181 (*)\n",
      "[Epoch 12/30, Step 4800, ETA 31.07s] step time: 0.003411s (Â±0.01032s); valid time: 0.104s; loss: -20.2429 (Â±6.57341); valid loss: 5.75601\n",
      "[Epoch 12/30, Step 4900, ETA 30.58s] step time: 0.003379s (Â±0.01022s); valid time: 0.103s; loss: -19.4541 (Â±6.42714); valid loss: 10.6899\n",
      "[Epoch 13/30, Step 5000, ETA 30.28s] step time: 0.003772s (Â±0.01559s); valid time: 0.1568s; loss: -18.5312 (Â±5.66878); valid loss: 5.08345 (*)\n",
      "[Epoch 13/30, Step 5100, ETA 29.8s] step time: 0.003259s (Â±0.01006s); valid time: 0.1013s; loss: -20.2323 (Â±6.69555); valid loss: 5.37645\n",
      "[Epoch 13/30, Step 5200, ETA 29.38s] step time: 0.003824s (Â±0.01494s); valid time: 0.1494s; loss: -21.2674 (Â±5.71384); valid loss: 5.00705 (*)\n",
      "[Epoch 13/30, Step 5300, ETA 28.9s] step time: 0.0033s (Â±0.01013s); valid time: 0.102s; loss: -21.0365 (Â±5.7206); valid loss: 7.15221\n",
      "[Epoch 14/30, Step 5400, ETA 28.51s] step time: 0.003231s (Â±0.01004s); valid time: 0.101s; loss: -20.7739 (Â±5.73646); valid loss: 6.55447\n",
      "[Epoch 14/30, Step 5500, ETA 28.04s] step time: 0.003288s (Â±0.01015s); valid time: 0.1012s; loss: -21.8166 (Â±5.25581); valid loss: 7.88806\n",
      "[Epoch 14/30, Step 5600, ETA 27.57s] step time: 0.003273s (Â±0.01014s); valid time: 0.1011s; loss: -21.1388 (Â±5.73545); valid loss: 5.378\n",
      "[Epoch 14/30, Step 5700, ETA 27.17s] step time: 0.003758s (Â±0.01488s); valid time: 0.1498s; loss: -20.4114 (Â±5.96446); valid loss: 4.53203 (*)\n",
      "[Epoch 15/30, Step 5800, ETA 26.79s] step time: 0.00349s (Â±0.01152s); valid time: 0.115s; loss: -20.0295 (Â±5.34903); valid loss: 5.35012\n",
      "[Epoch 15/30, Step 5900, ETA 26.35s] step time: 0.003381s (Â±0.01142s); valid time: 0.114s; loss: -20.4814 (Â±5.51721); valid loss: 7.66307\n",
      "[Epoch 15/30, Step 6000, ETA 25.9s] step time: 0.003342s (Â±0.01134s); valid time: 0.1141s; loss: -18.9563 (Â±6.48781); valid loss: 6.15428\n",
      "[Epoch 15/30, Step 6100, ETA 25.46s] step time: 0.003395s (Â±0.01137s); valid time: 0.1144s; loss: -20.7384 (Â±5.31075); valid loss: 6.75584\n",
      "[Epoch 16/30, Step 6200, ETA 25.09s] step time: 0.00342s (Â±0.01152s); valid time: 0.114s; loss: -20.2271 (Â±5.32163); valid loss: 6.67123\n",
      "[Epoch 16/30, Step 6300, ETA 24.65s] step time: 0.003435s (Â±0.01152s); valid time: 0.115s; loss: -20.4224 (Â±5.47741); valid loss: 9.86272\n",
      "[Epoch 16/30, Step 6400, ETA 24.22s] step time: 0.00341s (Â±0.01152s); valid time: 0.116s; loss: -21.2436 (Â±6.28736); valid loss: 8.70732\n",
      "[Epoch 16/30, Step 6500, ETA 23.82s] step time: 0.003867s (Â±0.0161s); valid time: 0.161s; loss: -22.2856 (Â±6.23858); valid loss: 4.27481 (*)\n",
      "[Epoch 17/30, Step 6600, ETA 23.44s] step time: 0.003433s (Â±0.01122s); valid time: 0.113s; loss: -20.0178 (Â±6.01495); valid loss: 14.059\n",
      "[Epoch 17/30, Step 6700, ETA 23s] step time: 0.003387s (Â±0.011s); valid time: 0.1107s; loss: -18.3211 (Â±7.0291); valid loss: 7.31013\n",
      "[Epoch 17/30, Step 6800, ETA 22.57s] step time: 0.003421s (Â±0.01142s); valid time: 0.114s; loss: -21.1464 (Â±5.67812); valid loss: 5.23695\n",
      "[Epoch 17/30, Step 6900, ETA 22.14s] step time: 0.003418s (Â±0.01135s); valid time: 0.1143s; loss: -21.0883 (Â±6.04423); valid loss: 11.0741\n",
      "[Epoch 17/30, Step 7000, ETA 21.71s] step time: 0.00344s (Â±0.01134s); valid time: 0.1142s; loss: -19.2855 (Â±6.21898); valid loss: 5.307\n",
      "[Epoch 18/30, Step 7100, ETA 21.33s] step time: 0.003497s (Â±0.01224s); valid time: 0.1222s; loss: -22.1835 (Â±6.12789); valid loss: 4.86763\n",
      "[Epoch 18/30, Step 7200, ETA 20.92s] step time: 0.00361s (Â±0.01271s); valid time: 0.127s; loss: -20.3068 (Â±6.0807); valid loss: 6.83607\n",
      "[Epoch 18/30, Step 7300, ETA 20.49s] step time: 0.003501s (Â±0.01223s); valid time: 0.1231s; loss: -20.5398 (Â±5.58028); valid loss: 4.64227\n",
      "[Epoch 18/30, Step 7400, ETA 20.08s] step time: 0.00357s (Â±0.01281s); valid time: 0.1269s; loss: -19.8815 (Â±6.03352); valid loss: 6.92444\n",
      "[Epoch 19/30, Step 7500, ETA 19.69s] step time: 0.003429s (Â±0.01152s); valid time: 0.115s; loss: -20.1387 (Â±6.2022); valid loss: 4.70634\n",
      "[Epoch 19/30, Step 7600, ETA 19.27s] step time: 0.003447s (Â±0.01152s); valid time: 0.116s; loss: -20.2566 (Â±6.73388); valid loss: 7.57844\n",
      "[Epoch 19/30, Step 7700, ETA 18.85s] step time: 0.003382s (Â±0.01154s); valid time: 0.1161s; loss: -21.9543 (Â±6.13325); valid loss: 6.29374\n",
      "[Epoch 19/30, Step 7800, ETA 18.42s] step time: 0.003423s (Â±0.01152s); valid time: 0.116s; loss: -16.3251 (Â±9.02338); valid loss: 5.80855\n",
      "[Epoch 20/30, Step 7900, ETA 18.03s] step time: 0.003321s (Â±0.009928s); valid time: 0.1s; loss: -19.8864 (Â±7.24703); valid loss: 7.51012\n",
      "[Epoch 20/30, Step 8000, ETA 17.6s] step time: 0.003281s (Â±0.01003s); valid time: 0.101s; loss: -20.7944 (Â±7.77755); valid loss: 19.77\n",
      "[Epoch 20/30, Step 8100, ETA 17.18s] step time: 0.003283s (Â±0.01013s); valid time: 0.102s; loss: -20.4086 (Â±7.38788); valid loss: 4.74\n",
      "[Epoch 20/30, Step 8200, ETA 16.75s] step time: 0.003361s (Â±0.01023s); valid time: 0.102s; loss: -19.7652 (Â±6.55901); valid loss: 6.6561\n",
      "[Epoch 20/30, Step 8240, ETA 16.56s] Learning rate decreased to 0.0005625000000000001\n",
      "[Epoch 21/30, Step 8300, ETA 16.37s] step time: 0.00338s (Â±0.01153s); valid time: 0.116s; loss: -20.1903 (Â±5.09633); valid loss: 5.24698\n",
      "[Epoch 21/30, Step 8400, ETA 15.95s] step time: 0.003491s (Â±0.01172s); valid time: 0.118s; loss: -23.4327 (Â±5.75643); valid loss: 4.44477\n",
      "[Epoch 21/30, Step 8500, ETA 15.53s] step time: 0.00342s (Â±0.01122s); valid time: 0.113s; loss: -22.9569 (Â±5.41464); valid loss: 4.62634\n",
      "[Epoch 21/30, Step 8600, ETA 15.14s] step time: 0.004006s (Â±0.01635s); valid time: 0.1636s; loss: -21.8633 (Â±6.29595); valid loss: 3.45909 (*)\n",
      "[Epoch 22/30, Step 8700, ETA 14.76s] step time: 0.00341s (Â±0.01112s); valid time: 0.112s; loss: -22.843 (Â±6.27997); valid loss: 4.86556\n",
      "[Epoch 22/30, Step 8800, ETA 14.34s] step time: 0.00346s (Â±0.01112s); valid time: 0.112s; loss: -23.0809 (Â±6.28254); valid loss: 5.93727\n",
      "[Epoch 22/30, Step 8900, ETA 13.93s] step time: 0.003352s (Â±0.01103s); valid time: 0.11s; loss: -22.2589 (Â±6.30273); valid loss: 4.85816\n",
      "[Epoch 22/30, Step 9000, ETA 13.52s] step time: 0.003411s (Â±0.01092s); valid time: 0.11s; loss: -22.3307 (Â±5.97335); valid loss: 5.38765\n",
      "[Epoch 23/30, Step 9100, ETA 13.12s] step time: 0.00333s (Â±0.01033s); valid time: 0.103s; loss: -23.8072 (Â±5.54359); valid loss: 8.86193\n",
      "[Epoch 23/30, Step 9200, ETA 12.71s] step time: 0.003461s (Â±0.01232s); valid time: 0.123s; loss: -22.484 (Â±5.91122); valid loss: 7.4371\n",
      "[Epoch 23/30, Step 9300, ETA 12.3s] step time: 0.003497s (Â±0.01208s); valid time: 0.1206s; loss: -23.0704 (Â±6.21964); valid loss: 4.98377\n",
      "[Epoch 23/30, Step 9400, ETA 11.9s] step time: 0.00348s (Â±0.01216s); valid time: 0.1224s; loss: -22.1518 (Â±6.40098); valid loss: 6.05404\n",
      "[Epoch 24/30, Step 9500, ETA 11.51s] step time: 0.003467s (Â±0.01158s); valid time: 0.1156s; loss: -23.2459 (Â±6.39855); valid loss: 7.03756\n",
      "[Epoch 24/30, Step 9600, ETA 11.09s] step time: 0.00341s (Â±0.01182s); valid time: 0.119s; loss: -22.6264 (Â±6.45217); valid loss: 5.23319\n",
      "[Epoch 24/30, Step 9700, ETA 10.68s] step time: 0.00347s (Â±0.01144s); valid time: 0.1153s; loss: -22.4929 (Â±6.04012); valid loss: 8.96839\n",
      "[Epoch 24/30, Step 9800, ETA 10.27s] step time: 0.00347s (Â±0.01192s); valid time: 0.12s; loss: -23.7929 (Â±6.15585); valid loss: 5.73677\n",
      "[Epoch 25/30, Step 9900, ETA 9.88s] step time: 0.003344s (Â±0.01113s); valid time: 0.112s; loss: -22.6879 (Â±5.8403); valid loss: 3.53141\n",
      "[Epoch 25/30, Step 10000, ETA 9.469s] step time: 0.003336s (Â±0.01078s); valid time: 0.1075s; loss: -22.1675 (Â±6.27981); valid loss: 4.16627\n",
      "[Epoch 25/30, Step 10100, ETA 9.058s] step time: 0.003292s (Â±0.01014s); valid time: 0.1021s; loss: -21.3835 (Â±5.61691); valid loss: 8.58374\n",
      "[Epoch 25/30, Step 10200, ETA 8.647s] step time: 0.003298s (Â±0.01016s); valid time: 0.1013s; loss: -24.0258 (Â±5.88988); valid loss: 6.79204\n",
      "[Epoch 25/30, Step 10300, ETA 8.239s] step time: 0.003266s (Â±0.01014s); valid time: 0.102s; loss: -23.9415 (Â±5.90601); valid loss: 9.90258\n",
      "[Epoch 26/30, Step 10400, ETA 7.844s] step time: 0.003411s (Â±0.01088s); valid time: 0.1085s; loss: -23.0687 (Â±6.18692); valid loss: 7.56793\n",
      "[Epoch 26/30, Step 10500, ETA 7.438s] step time: 0.00339s (Â±0.01093s); valid time: 0.11s; loss: -23.6356 (Â±6.15661); valid loss: 5.17748\n",
      "[Epoch 26/30, Step 10600, ETA 7.032s] step time: 0.00339s (Â±0.01082s); valid time: 0.109s; loss: -24.1155 (Â±6.39168); valid loss: 6.37385\n",
      "[Epoch 26/30, Step 10700, ETA 6.626s] step time: 0.003345s (Â±0.01087s); valid time: 0.1094s; loss: -23.3489 (Â±5.62679); valid loss: 6.51031\n",
      "[Epoch 27/30, Step 10800, ETA 6.233s] step time: 0.003441s (Â±0.01162s); valid time: 0.116s; loss: -22.8964 (Â±5.09693); valid loss: 11.7121\n",
      "[Epoch 27/30, Step 10900, ETA 5.83s] step time: 0.003421s (Â±0.01192s); valid time: 0.12s; loss: -23.1179 (Â±6.03784); valid loss: 4.46717\n",
      "[Epoch 27/30, Step 11000, ETA 5.427s] step time: 0.00335s (Â±0.01183s); valid time: 0.119s; loss: -23.9307 (Â±5.24906); valid loss: 3.83389\n",
      "[Epoch 27/30, Step 11100, ETA 5.025s] step time: 0.003357s (Â±0.0119s); valid time: 0.1187s; loss: -24.589 (Â±5.76281); valid loss: 4.51455\n",
      "[Epoch 28/30, Step 11200, ETA 4.628s] step time: 0.003312s (Â±0.01013s); valid time: 0.102s; loss: -24.2855 (Â±5.60965); valid loss: 5.4738\n",
      "[Epoch 28/30, Step 11300, ETA 4.225s] step time: 0.003232s (Â±0.01015s); valid time: 0.1011s; loss: -23.2318 (Â±6.70701); valid loss: 15.1726\n",
      "[Epoch 28/30, Step 11400, ETA 3.824s] step time: 0.00337s (Â±0.01063s); valid time: 0.107s; loss: -23.6674 (Â±5.95695); valid loss: 39.9364\n",
      "[Epoch 28/30, Step 11500, ETA 3.423s] step time: 0.003351s (Â±0.01073s); valid time: 0.108s; loss: -24.2 (Â±6.23217); valid loss: 14.0345\n",
      "[Epoch 29/30, Step 11600, ETA 3.026s] step time: 0.00335s (Â±0.01033s); valid time: 0.104s; loss: -23.8997 (Â±6.31241); valid loss: 20.7985\n",
      "[Epoch 29/30, Step 11700, ETA 2.627s] step time: 0.003476s (Â±0.01158s); valid time: 0.1166s; loss: -22.7682 (Â±6.88921); valid loss: 77.5074\n",
      "[Epoch 29/30, Step 11800, ETA 2.227s] step time: 0.003323s (Â±0.01085s); valid time: 0.1092s; loss: -23.7968 (Â±6.48187); valid loss: 95.0217\n",
      "[Epoch 29/30, Step 11900, ETA 1.828s] step time: 0.0035s (Â±0.01172s); valid time: 0.117s; loss: -24.6815 (Â±6.44054); valid loss: 9.80001\n",
      "[Epoch 30/30, Step 12000, ETA 1.432s] step time: 0.00342s (Â±0.01182s); valid time: 0.118s; loss: -23.437 (Â±5.26321); valid loss: 12.4091\n",
      "[Epoch 30/30, Step 12100, ETA 1.034s] step time: 0.003509s (Â±0.01191s); valid time: 0.1199s; loss: -24.7533 (Â±6.05777); valid loss: 10.4379\n",
      "[Epoch 30/30, Step 12200, ETA 0.6359s] step time: 0.003486s (Â±0.01172s); valid time: 0.118s; loss: -23.4712 (Â±5.70304); valid loss: 236.35\n",
      "[Epoch 30/30, Step 12300, ETA 0.2383s] step time: 0.003326s (Â±0.01112s); valid time: 0.1109s; loss: -23.4762 (Â±5.6701); valid loss: 27.4205\n",
      "[Epoch 30/30, Step 12360, ETA 0s] Learning rate decreased to 0.00042187500000000005\n",
      "Number of predictions: 64681\n",
      "-- Final Results --\n",
      "Best anomaly threshold 5.800000000000001\n",
      "Anomalies found: 5445/6334\n",
      "-- Anomaly Rows --\n",
      "Precision: 0.874\n",
      "Recall: 0.766\n",
      "F1-score: 0.816\n"
     ]
    }
   ],
   "source": [
    "#Using the DonutTrainer class to train the model \n",
    "trainer = DonutTrainer(model=model, model_vs=model_vs, max_epoch=30)\n",
    "\n",
    "predictor = DonutPredictor(model)\n",
    "\n",
    "with tf.Session().as_default(): \n",
    "    trainer.fit(train_values, train_labels, train_missing, mean, std)\n",
    "    #Making predictions\n",
    "    test_score = predictor.get_score(test_values, test_missing)\n",
    "    print(\"Number of predictions: {}\".format(test_score.shape[0]))\n",
    "    # try different thresholds\n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    best_predictions = []\n",
    "    thresholds = np.arange(5, 50, 0.2)\n",
    "    for t in thresholds:\n",
    "        threshold = t  # can be changed to better fit the training data\n",
    "        anomaly_predictions = []\n",
    "        for l in test_score:\n",
    "            if abs(l) > threshold:\n",
    "                anomaly_predictions.append(1)\n",
    "            else:\n",
    "                anomaly_predictions.append(0)\n",
    "        for i in range(sliding_window-1, len(anomaly_predictions)):\n",
    "            if anomaly_predictions[i-sliding_window+1] == 1 and test_labels[i] == 1:  # true positive\n",
    "                j = i-1\n",
    "                while j >= sliding_window-1 and test_labels[j] == 1\\\n",
    "                        and anomaly_predictions[j-sliding_window+1] == 0:\n",
    "                    anomaly_predictions[j-sliding_window+1] = 1\n",
    "                    j -= 1\n",
    "                j = i+1\n",
    "                while j < len(anomaly_predictions) and test_labels[j] == 1\\\n",
    "                        and anomaly_predictions[j-sliding_window+1] == 0:\n",
    "                    anomaly_predictions[j-sliding_window+1] = 1\n",
    "                    j += 1\n",
    "        f1 = f1_score(test_labels[sliding_window-1:], anomaly_predictions, average='binary')\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_predictions = anomaly_predictions\n",
    "\n",
    "    anomaly_predictions = np.array(best_predictions)\n",
    "    print(\"-- Final Results --\")\n",
    "    print(\"Best anomaly threshold {}\".format(best_threshold))\n",
    "    print(\"Anomalies found: {}/{}\".format(np.sum(anomaly_predictions == 1), np.sum(test_labels == 1)))\n",
    "    prfs = precision_recall_fscore_support(test_labels[sliding_window-1:], anomaly_predictions)\n",
    "    print(\"-- Anomaly Rows --\")\n",
    "    print(\"Precision: {:.3f}\".format(prfs[0][1]))\n",
    "    print(\"Recall: {:.3f}\".format(prfs[1][1]))\n",
    "    print(\"F1-score: {:.3f}\".format(prfs[2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
